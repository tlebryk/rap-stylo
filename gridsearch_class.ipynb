{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from time import time\n",
    "# import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import functools\n",
    "# from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "# from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape, Embedding, GlobalMaxPooling1D, Conv1D\n",
    "from keras.optimizers import SGD \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes path, returns list with of albums in json format\n",
    "# each json entry is a song with lyrics and other metadata\n",
    "def json_extract(path):\n",
    "    data_list=[]\n",
    "    for file in os.listdir(path): \n",
    "        if file[-5:] == '.json':\n",
    "            with open(path+file, 'r') as f: \n",
    "                data = json.load(f)\n",
    "                data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/drake/'\n",
    "drake=json_extract(path)\n",
    "path='data/quentin_miller/'\n",
    "quentin=json_extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove obvious identifiers and stem words\n",
    "stops={'drizzy', 'drake', 'quentin', 'miller', 'ovo', 'champagne', 'papi','toronto', 'atlanta', '6'}\n",
    "analyze = CV().build_analyzer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stop_removal(lyrics : str): \n",
    "    toks=analyze(lyrics)\n",
    "    return ' '.join([ps.stem(word) for word in toks if not ps.stem(word) in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resplit the data into train and test \n",
    "np.random.seed(random_seed)\n",
    "X_train = []\n",
    "train_titles=[]\n",
    "iyrtitl_lyrics= []\n",
    "iyrtitl_titles= []\n",
    "X_test=[]\n",
    "test_titles=[]\n",
    "#count number of drake songs in train, test set\n",
    "d_train_cnt=0\n",
    "d_test_cnt=0\n",
    "for album in drake: \n",
    "    for song in album: \n",
    "        # keep track of \"If you're...\" to put in test set (iytitl is subject to ambiguous authorship)\n",
    "        if song[\"album\"]==\"If You’re Reading This It’s Too Late \":\n",
    "            iyrtitl_titles.append(song['title'])\n",
    "            iyrtitl_lyrics.append(stop_removal(song['lyrics']))\n",
    "        # oversample from Drake to balance training sample further. \n",
    "        elif np.random.random(1) < .15:\n",
    "            test_titles.append(song['title'])\n",
    "            X_test.append(stop_removal(song['lyrics']))   \n",
    "            d_test_cnt+=1\n",
    "        else: \n",
    "            train_titles.append(song['title'])\n",
    "            X_train.append(stop_removal(song['lyrics']))\n",
    "            d_train_cnt+=1\n",
    "\n",
    "for album in quentin: \n",
    "    for song in album: \n",
    "        if np.random.random(1) < .1:\n",
    "            test_titles.append(song['title'])\n",
    "            X_test.append(stop_removal(song['lyrics']))   \n",
    "        else: \n",
    "            train_titles.append(song['title'])\n",
    "            X_train.append(stop_removal(song['lyrics']))\n",
    "\n",
    "# label drake as 0 and Quentin Miller as 1 in y column. \n",
    "y_train=np.zeros(len(X_train))    \n",
    "y_train[d_train_cnt:]=1\n",
    "y_test=np.zeros(len(X_test))\n",
    "y_test[d_test_cnt:]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of validation accuracy of all models (NOT held out test set)\n",
    "score_dict = {}\n",
    "#track the cross-validated best estimator. \n",
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline preprocessing values to cv on. \n",
    "# commented out parameters performed better across the board or didn't provide much improvement so they have been moved to preprocessing or the default transformer\n",
    "pipe_base = [ ('vect', CV(max_df=.5, ngram_range= (1, 2))), ('tfidf', TfidfTransformer())]\n",
    "param_base = {   \n",
    "#     'vect__max_df': (.4, .5), \n",
    "    'vect__min_df': (0.002, 0.005, 0.007), \n",
    "#     'vect__ngram_range' : (1, 2), (1,3)),\n",
    "#     'vect__analyzer' : ('word', stemming),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a pipeline, hyperparameters, and a number of folds. \n",
    "# prints information about the grid search and returns the GridSearchCV object with the best model\n",
    "def grid_search(pipeline, param, k=6):\n",
    "    model = RandomizedSearchCV(pipeline, param, random_state=random_seed, \n",
    "                              cv=k, n_iter=15, verbose =0)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\", param)\n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - start))\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    for param_name in sorted(param.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, model.best_params_[param_name]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds results from gridsearch to global variables model_dict and score_dict \n",
    "# key should be a three letter abbreviation for the model's name. \n",
    "def track_model (key, pipeline, param, k=5):\n",
    "    global model_dict\n",
    "    global score_dict\n",
    "    grid = grid_search(pipeline, param, k)\n",
    "    model_dict[key] = grid.best_estimator_\n",
    "    score_dict[key] = grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=\n",
    "                    pipe_base + [('clf', LogisticRegression(class_weight='balanced', \n",
    "                               random_state=random_seed, \n",
    "                               penalty='elasticnet', \n",
    "                               solver='saga'))])\n",
    "params= param_base.copy()\n",
    "params['clf__l1_ratio'] = (0, 0.1, 0.5, 0.9)\n",
    "params['clf__C'] = (0.1, 1., 10, 100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__l1_ratio': (0, 0.1, 0.5, 0.9), 'clf__C': (0.1, 1.0, 10, 100, 1000)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 26.373s\n",
      "Best score: 0.871\n",
      "Best parameters set:\n",
      "\tclf__C: 1000\n",
      "\tclf__l1_ratio: 0.1\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__min_df: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "track_model('log', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use modified_huber to ensure we get probability estimates\n",
    "pipeline = Pipeline(steps=pipe_base + [('clf', SGDClassifier(class_weight='balanced', \n",
    "                          random_state=random_seed,  loss= 'modified_huber'))])\n",
    "params= param_base.copy()\n",
    "params['clf__alpha'] = (0.0001, 0.001, 0.01)\n",
    "params['clf__l1_ratio'] =(0, 0.5, 0.75, 1)\n",
    "params['clf__tol'] = (0.0001, 0.001)\n",
    "# params['clf__loss'] =  (‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': (0.0001, 0.001, 0.01), 'clf__l1_ratio': (0, 0.5, 0.75, 1), 'clf__tol': (0.0001, 0.001)}\n",
      "done in 17.108s\n",
      "Best score: 0.858\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tclf__l1_ratio: 1\n",
      "\tclf__tol: 0.0001\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__min_df: 0.002\n"
     ]
    }
   ],
   "source": [
    "track_model('sgd', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear kernel to ensure probability estimates (however we can uncomment and delete to CV different kernels)\n",
    "pipeline = Pipeline(steps=\n",
    "    pipe_base + [('clf', SVC(kernel='linear', random_state=random_seed, \n",
    "                             class_weight='balanced', probability=True))])\n",
    "\n",
    "params = param_base.copy()\n",
    "# params['clf__kernel'] = ('linear', 'poly', 'rbf')\n",
    "params['clf__C'] = (0.1, 0.5, 0.75, 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__C': (0.1, 0.5, 0.75, 0.9)}\n",
      "done in 57.314s\n",
      "Best score: 0.871\n",
      "Best parameters set:\n",
      "\tclf__C: 0.9\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__min_df: 0.007\n"
     ]
    }
   ],
   "source": [
    "track_model('svc', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=\n",
    "    pipe_base + [('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "\n",
    "params= param_base.copy()\n",
    "params['clf__n_estimators'] = (100, 150, 175) \n",
    "# params['clf__max_depth'] = (30, 50, 100)\n",
    "params['clf__max_features'] = ('log2', 'auto')\n",
    "# params['clf__min_samples_split'] = (2, 4, 8)\n",
    "\n",
    "# params['clf__max_leaf_nodes'] = (5, 25, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__n_estimators': (100, 150, 175), 'clf__max_features': ('log2', 'auto')}\n",
      "done in 33.678s\n",
      "Best score: 0.828\n",
      "Best parameters set:\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__n_estimators: 100\n",
      "\ttfidf__norm: 'l1'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__min_df: 0.007\n"
     ]
    }
   ],
   "source": [
    "track_model('rfc', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=\n",
    "    pipe_base+[('clf', AdaBoostClassifier(random_state=random_seed))])\n",
    "\n",
    "params = param_base.copy()\n",
    "params['clf__base_estimator'] = (DTC(max_depth=1), DTC(max_depth=2), DTC(max_depth=4))\n",
    "params['clf__n_estimators'] = (100, 120, 140)\n",
    "params['clf__learning_rate'] = (0.2, 0.25, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__base_estimator': (DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), DecisionTreeClassifier(max_depth=4)), 'clf__n_estimators': (100, 120, 140), 'clf__learning_rate': (0.2, 0.25, 0.3)}\n",
      "done in 146.510s\n",
      "Best score: 0.825\n",
      "Best parameters set:\n",
      "\tclf__base_estimator: DecisionTreeClassifier(max_depth=1)\n",
      "\tclf__learning_rate: 0.25\n",
      "\tclf__n_estimators: 120\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__min_df: 0.002\n"
     ]
    }
   ],
   "source": [
    "track_model('ada', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=\n",
    "                    pipe_base + [('clf', GradientBoostingClassifier(random_state=random_seed))])\n",
    "\n",
    "params = param_base.copy()\n",
    "params['clf__learning_rate'] =(0.15, 0.2, 0.25)\n",
    "params['clf__n_estimators'] = (50, 75, 100)\n",
    "params['clf__min_samples_leaf']= (1,2,5)\n",
    "params['clf__max_depth'] = (2,3,5)\n",
    "params['clf__min_impurity_decrease'] =  (0, 0.01, 0.05)\n",
    "params['clf__tol'] =  (0.0001, 0.001, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (0.002, 0.005, 0.007), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__learning_rate': (0.15, 0.2, 0.25), 'clf__n_estimators': (50, 75, 100), 'clf__min_samples_leaf': (1, 2, 5), 'clf__max_depth': (2, 3, 5), 'clf__min_impurity_decrease': (0, 0.01, 0.05), 'clf__tol': (0.0001, 0.001, 0.005)}\n",
      "done in 93.989s\n",
      "Best score: 0.837\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.2\n",
      "\tclf__max_depth: 2\n",
      "\tclf__min_impurity_decrease: 0\n",
      "\tclf__min_samples_leaf: 5\n",
      "\tclf__n_estimators: 100\n",
      "\tclf__tol: 0.005\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__min_df: 0.007\n"
     ]
    }
   ],
   "source": [
    "track_model('gbc', pipeline, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on training data\n",
    "maxlen = 700\n",
    "num_words=3000\n",
    "tokenizer=Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus into word embeddings\n",
    "Xcnn_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=maxlen)\n",
    "Xcnn_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=maxlen)\n",
    "Xcnn_iyrtest = pad_sequences(tokenizer.texts_to_sequences(iyrtitl_lyrics), maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', min_delta=0.0005, \n",
    "                                              patience=10, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid_search(build_fn, params, k, epochs, batch_size, n_iter): \n",
    "    model = KerasClassifier(build_fn=build_fn, verbose=False)\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions=params, cv=k, verbose=1, n_iter=n_iter)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"parameters:\", params)\n",
    "    start = time()\n",
    "    result = grid.fit(Xcnn_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[earlyStopping])\n",
    "    print(\"done in %0.3fs\" % (time() - start))\n",
    "    print(\"Best score: %0.3f\" % result.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    for param_name in sorted(params.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, result.best_params_[param_name]))\n",
    "    return result\n",
    "    \n",
    "    \n",
    "#     test_accuracy= grid.score(X_test, y_test)\n",
    "#     print(\"validation accuracy of best model: {}\\n\".format(test_accuracy))\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node layers currently only uses the first 2 elements as number of nodes in dense layers.\n",
    "def build_fn(node_layer, num_filters, kernel_size, vocab_size, embedding_dim, maxlen): \n",
    "    model = Sequential([\n",
    "        Embedding(input_dim = vocab_size, \n",
    "              output_dim = embedding_dim,\n",
    "              input_length=maxlen),\n",
    "        Conv1D(num_filters, kernel_size, activation='relu'),\n",
    "        GlobalMaxPooling1D(), \n",
    "        Dense(node_layer, activation='relu'),\n",
    "        Dense(node_layer, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='sgd', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "params= {\n",
    "    'node_layer' : [25,10],\n",
    "    'num_filters' : [32, 64, 128],\n",
    "    'kernel_size':[3,5,7],\n",
    "    'vocab_size' : [vocab_size],\n",
    "    'embedding_dim' : [embedding_dim],\n",
    "    'maxlen' :[maxlen],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "parameters: {'node_layer': [25, 10], 'num_filters': [32, 64, 128], 'kernel_size': [3, 5, 7], 'vocab_size': [5466], 'embedding_dim': [50], 'maxlen': [700]}\n",
      "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 19.6min finished\n",
      "C:\\Users\\Tyler\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1187.009s\n",
      "Best score: 0.630\n",
      "Best parameters set:\n",
      "\tembedding_dim: 50\n",
      "\tkernel_size: 3\n",
      "\tmaxlen: 700\n",
      "\tnode_layer: 25\n",
      "\tnum_filters: 32\n",
      "\tvocab_size: 5466\n"
     ]
    }
   ],
   "source": [
    "# keeping large batch size to keep training time manageable\n",
    "epochs = 50\n",
    "batch_size=32\n",
    "cnn_model=random_grid_search(build_fn, params, 6, epochs, batch_size, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['cnn']=cnn_model.best_score_\n",
    "# score_dict['cnn']=cnn_model.accuracy\n",
    "# model_dict['cnn'] = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produces the average of prediction probabilities all the models (unweighted)\n",
    "# We've removed the machine learning model from this method because it doesn't improve over the baseline accuracy. \n",
    "def ensemble_proba(corpus): \n",
    "    arr=np.zeros((len(corpus), 2*(len(model_dict))))\n",
    "    preds = np.zeros(len(corpus))\n",
    "    for ind, key in enumerate(model_dict): \n",
    "        corp = model_dict[key]['vect'].transform(corpus)\n",
    "        corp = model_dict[key]['tfidf'].transform(corp)\n",
    "        probs = model_dict[key]['clf'].predict_proba(corp)\n",
    "        arr[:, 2*ind]=probs[:, 0]\n",
    "        arr[:, 2*ind+1]=-probs[:, 1]\n",
    "#     corp=pad_sequences(tokenizer.texts_to_sequences(corpus), maxlen=maxlen)\n",
    "#     probs =cnn_model.best_estimator_.predict_proba(corp)\n",
    "#     arr[:,-2]=probs[:,0]\n",
    "#     arr[:,-1]=-probs[:,1]\n",
    "    preds = np.sum(arr, axis=1)\n",
    "    return(preds/(2*len(model_dict))+.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts using a majority wins approach. \n",
    "# note: ties currently are handled sloppily\n",
    "def ensemble_pred(corpus): \n",
    "    arr=np.zeros((len(corpus), (len(model_dict))))\n",
    "    preds = np.zeros(len(corpus))\n",
    "    for ind, key in enumerate(model_dict): \n",
    "        corp = model_dict[key]['vect'].transform(corpus)\n",
    "        corp = model_dict[key]['tfidf'].transform(corp)\n",
    "        arr[:, ind] = model_dict[key]['clf'].predict(corp)\n",
    "#         arr[:, 2*ind]=probs[:, 0]\n",
    "#         arr[:, 2*ind+1]=-probs[:, 1]\n",
    "#     corp=pad_sequences(tokenizer.texts_to_sequences(corpus), maxlen=maxlen)\n",
    "#     arr[:,-1] =cnn_model.best_estimator_.predict(corp)[:, 0]\n",
    "    preds=np.sum(arr, axis=1)\n",
    "    f = lambda x: 1 if (x 2 3) else 0\n",
    "    return np.array([f(pred) for pred in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log  train: 0.99\n",
      "log  test: 0.87\n",
      "sgd  train: 1.00\n",
      "sgd  test: 0.85\n",
      "svc  train: 1.00\n",
      "svc  test: 0.87\n",
      "rfc  train: 1.00\n",
      "rfc  test: 0.87\n",
      "ada  train: 1.00\n",
      "ada  test: 0.85\n",
      "gbc  train: 1.00\n",
      "gbc  test: 0.85\n",
      "cnn train: 0.63\n",
      "cnn test: 0.67\n"
     ]
    }
   ],
   "source": [
    "df_models=pd.DataFrame(index=score_dict.keys(), columns = ['train', 'validation', 'test'])\n",
    "df_models['validation'] = score_dict.values()\n",
    "training=np.zeros(len(score_dict))\n",
    "testing=np.zeros(len(score_dict))\n",
    "for ind, key in enumerate(model_dict): \n",
    "    X_trans = model_dict[key]['vect'].transform(X_train)\n",
    "    X_trans = model_dict[key]['tfidf'].transform(X_trans)\n",
    "    print(key, \" train: {:.2f}\".format(accuracy_score(y_train, model_dict[key]['clf'].predict(X_trans))))\n",
    "    training[ind]=accuracy_score(y_train, model_dict[key]['clf'].predict(X_trans))\n",
    "    X_trans = model_dict[key]['vect'].transform(X_test)\n",
    "    X_trans = model_dict[key]['tfidf'].transform(X_trans)\n",
    "    testing[ind]=accuracy_score(y_test, model_dict[key]['clf'].predict(X_trans))\n",
    "# print indexes and values just to be safe\n",
    "    print(key, \" test: {:.2f}\".format(accuracy_score(y_test, model_dict[key]['clf'].predict(X_trans))))\n",
    "\n",
    "X_trans=pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=maxlen)\n",
    "training[-1]= accuracy_score(y_train, cnn_model.best_estimator_.predict(X_trans)) \n",
    "print(\"cnn train: {:.2f}\".format(accuracy_score(y_train, cnn_model.best_estimator_.predict(X_trans)))) \n",
    "X_trans=pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=maxlen)\n",
    "testing[-1]= accuracy_score(y_test, cnn_model.best_estimator_.predict(X_trans)) \n",
    "print(\"cnn test: {:.2f}\".format(accuracy_score(y_test, cnn_model.best_estimator_.predict(X_trans)))) \n",
    "df_models['train']=training\n",
    "df_models['test']=testing\n",
    "# df_models.append({'ens':[accuracy_score(y_train,ensemble_pred(X_train)), float('Nan'), accuracy_score(y_test,ensemble_pred(X_test))]})\n",
    "df2=pd.DataFrame(np.array([accuracy_score(y_train,ensemble_pred(X_train)), \n",
    "                  float('Nan'), \n",
    "                  accuracy_score(y_test,ensemble_pred(X_test))]).reshape(1,3), \n",
    "                     columns=['train', 'validation', 'test'], index=['ens'])\n",
    "df_models=df_models.append(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log</th>\n",
       "      <td>0.993846</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858462</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827692</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824615</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836923</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ens</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        train  validation      test\n",
       "log  0.993846    0.870769  0.872727\n",
       "sgd  1.000000    0.858462  0.854545\n",
       "svc  1.000000    0.870769  0.872727\n",
       "rfc  1.000000    0.827692  0.872727\n",
       "ada  1.000000    0.824615  0.854545\n",
       "gbc  1.000000    0.836923  0.854545\n",
       "cnn  0.630769    0.629630  0.672727\n",
       "ens  1.000000         NaN  0.909091"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is:  log\n"
     ]
    }
   ],
   "source": [
    "best_key=df_models.idxmax(axis=0)['validation']\n",
    "print(\"The best model is: \", best_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Predictions for *If You're Reading this it's Too Late* songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track the test set predictions for held out \"If You're Reading This its Too Late\" \n",
    "df_iyrtitl=pd.DataFrame(np.zeros(len(iyrtitl_titles)), index=iyrtitl_titles, columns=[\"credits\"])\n",
    "credits = ['10 Bands', \"Legend\", \"Know Yourself\", \"Used To\"]\n",
    "for name in credits:\n",
    "    df_iyrtitl.loc[name]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions/ stacking for iyrtitl\n",
    "for key in model_dict:\n",
    "    test = model_dict[key]['vect'].transform(iyrtitl_lyrics)\n",
    "    test = model_dict[key]['tfidf'].transform(test)\n",
    "    df_iyrtitl[key+'_drake']= model_dict[key]['clf'].predict_proba(test)[:,0]\n",
    "    df_iyrtitl[key+'_quen']= 1-df_iyrtitl[key+'_drake']\n",
    "df_iyrtitl['cnn_drake']=cnn_model.best_estimator_.predict_proba(Xcnn_iyrtest)[:,0]\n",
    "df_iyrtitl['cnn_quen']=1-df_iyrtitl['cnn_drake']\n",
    "df_iyrtitl['ens_drake']=ensemble_proba(iyrtitl_lyrics)\n",
    "df_iyrtitl['ens_quen']=1-df_iyrtitl['ens_drake']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "      <th>log_drake</th>\n",
       "      <th>log_quen</th>\n",
       "      <th>sgd_drake</th>\n",
       "      <th>sgd_quen</th>\n",
       "      <th>svc_drake</th>\n",
       "      <th>svc_quen</th>\n",
       "      <th>rfc_drake</th>\n",
       "      <th>rfc_quen</th>\n",
       "      <th>ada_drake</th>\n",
       "      <th>ada_quen</th>\n",
       "      <th>gbc_drake</th>\n",
       "      <th>gbc_quen</th>\n",
       "      <th>cnn_drake</th>\n",
       "      <th>cnn_quen</th>\n",
       "      <th>ens_drake</th>\n",
       "      <th>ens_quen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Legend</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.597164</td>\n",
       "      <td>0.402836</td>\n",
       "      <td>0.626774</td>\n",
       "      <td>0.373226</td>\n",
       "      <td>0.907117</td>\n",
       "      <td>0.092883</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.541413</td>\n",
       "      <td>0.458587</td>\n",
       "      <td>0.953654</td>\n",
       "      <td>0.046346</td>\n",
       "      <td>0.622125</td>\n",
       "      <td>0.377875</td>\n",
       "      <td>0.716020</td>\n",
       "      <td>0.283980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.529922</td>\n",
       "      <td>0.470078</td>\n",
       "      <td>0.627841</td>\n",
       "      <td>0.372159</td>\n",
       "      <td>0.811741</td>\n",
       "      <td>0.188259</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.558265</td>\n",
       "      <td>0.441735</td>\n",
       "      <td>0.855129</td>\n",
       "      <td>0.144871</td>\n",
       "      <td>0.628946</td>\n",
       "      <td>0.371054</td>\n",
       "      <td>0.665483</td>\n",
       "      <td>0.334517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Bands</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455955</td>\n",
       "      <td>0.544045</td>\n",
       "      <td>0.431652</td>\n",
       "      <td>0.568348</td>\n",
       "      <td>0.084154</td>\n",
       "      <td>0.915846</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.473329</td>\n",
       "      <td>0.526671</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.626616</td>\n",
       "      <td>0.373384</td>\n",
       "      <td>0.368256</td>\n",
       "      <td>0.631744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know Yourself</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550921</td>\n",
       "      <td>0.449079</td>\n",
       "      <td>0.613523</td>\n",
       "      <td>0.386477</td>\n",
       "      <td>0.849871</td>\n",
       "      <td>0.150129</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.614490</td>\n",
       "      <td>0.385510</td>\n",
       "      <td>0.937369</td>\n",
       "      <td>0.062631</td>\n",
       "      <td>0.624701</td>\n",
       "      <td>0.375299</td>\n",
       "      <td>0.702696</td>\n",
       "      <td>0.297304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Tellin’</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471311</td>\n",
       "      <td>0.528689</td>\n",
       "      <td>0.468214</td>\n",
       "      <td>0.531786</td>\n",
       "      <td>0.120009</td>\n",
       "      <td>0.879991</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.494069</td>\n",
       "      <td>0.505931</td>\n",
       "      <td>0.366704</td>\n",
       "      <td>0.633296</td>\n",
       "      <td>0.624795</td>\n",
       "      <td>0.375205</td>\n",
       "      <td>0.425051</td>\n",
       "      <td>0.574949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madonna</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571905</td>\n",
       "      <td>0.428095</td>\n",
       "      <td>0.719589</td>\n",
       "      <td>0.280411</td>\n",
       "      <td>0.952187</td>\n",
       "      <td>0.047813</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.557648</td>\n",
       "      <td>0.442352</td>\n",
       "      <td>0.932966</td>\n",
       "      <td>0.067034</td>\n",
       "      <td>0.623011</td>\n",
       "      <td>0.376989</td>\n",
       "      <td>0.745716</td>\n",
       "      <td>0.254284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 God</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572521</td>\n",
       "      <td>0.427479</td>\n",
       "      <td>0.683747</td>\n",
       "      <td>0.316253</td>\n",
       "      <td>0.920352</td>\n",
       "      <td>0.079648</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.566147</td>\n",
       "      <td>0.433853</td>\n",
       "      <td>0.768669</td>\n",
       "      <td>0.231331</td>\n",
       "      <td>0.625225</td>\n",
       "      <td>0.374775</td>\n",
       "      <td>0.696906</td>\n",
       "      <td>0.303094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star67</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536639</td>\n",
       "      <td>0.463361</td>\n",
       "      <td>0.703808</td>\n",
       "      <td>0.296192</td>\n",
       "      <td>0.929450</td>\n",
       "      <td>0.070550</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.494910</td>\n",
       "      <td>0.505090</td>\n",
       "      <td>0.655004</td>\n",
       "      <td>0.344996</td>\n",
       "      <td>0.628706</td>\n",
       "      <td>0.371294</td>\n",
       "      <td>0.661635</td>\n",
       "      <td>0.338365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preach</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564851</td>\n",
       "      <td>0.435149</td>\n",
       "      <td>0.663840</td>\n",
       "      <td>0.336160</td>\n",
       "      <td>0.878252</td>\n",
       "      <td>0.121748</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.536151</td>\n",
       "      <td>0.463849</td>\n",
       "      <td>0.681359</td>\n",
       "      <td>0.318641</td>\n",
       "      <td>0.623281</td>\n",
       "      <td>0.376719</td>\n",
       "      <td>0.647409</td>\n",
       "      <td>0.352591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Used To</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.576648</td>\n",
       "      <td>0.423352</td>\n",
       "      <td>0.765248</td>\n",
       "      <td>0.234752</td>\n",
       "      <td>0.975054</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.514420</td>\n",
       "      <td>0.485580</td>\n",
       "      <td>0.878076</td>\n",
       "      <td>0.121924</td>\n",
       "      <td>0.627075</td>\n",
       "      <td>0.372925</td>\n",
       "      <td>0.744908</td>\n",
       "      <td>0.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Man</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568942</td>\n",
       "      <td>0.431058</td>\n",
       "      <td>0.711573</td>\n",
       "      <td>0.288427</td>\n",
       "      <td>0.946372</td>\n",
       "      <td>0.053628</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.514141</td>\n",
       "      <td>0.485859</td>\n",
       "      <td>0.931333</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.625370</td>\n",
       "      <td>0.374630</td>\n",
       "      <td>0.715393</td>\n",
       "      <td>0.284607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Now &amp; Forever</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588651</td>\n",
       "      <td>0.411349</td>\n",
       "      <td>0.643662</td>\n",
       "      <td>0.356338</td>\n",
       "      <td>0.842335</td>\n",
       "      <td>0.157665</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.612740</td>\n",
       "      <td>0.387260</td>\n",
       "      <td>0.885292</td>\n",
       "      <td>0.114708</td>\n",
       "      <td>0.620968</td>\n",
       "      <td>0.379032</td>\n",
       "      <td>0.707113</td>\n",
       "      <td>0.292887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667774</td>\n",
       "      <td>0.332226</td>\n",
       "      <td>0.995657</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.999559</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.596438</td>\n",
       "      <td>0.403562</td>\n",
       "      <td>0.942788</td>\n",
       "      <td>0.057212</td>\n",
       "      <td>0.626480</td>\n",
       "      <td>0.373520</td>\n",
       "      <td>0.820369</td>\n",
       "      <td>0.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You &amp; The 6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612233</td>\n",
       "      <td>0.387767</td>\n",
       "      <td>0.907056</td>\n",
       "      <td>0.092944</td>\n",
       "      <td>0.995669</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.736355</td>\n",
       "      <td>0.263645</td>\n",
       "      <td>0.987269</td>\n",
       "      <td>0.012731</td>\n",
       "      <td>0.625742</td>\n",
       "      <td>0.374258</td>\n",
       "      <td>0.858097</td>\n",
       "      <td>0.141903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738207</td>\n",
       "      <td>0.261793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999863</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.551691</td>\n",
       "      <td>0.448309</td>\n",
       "      <td>0.978274</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.624620</td>\n",
       "      <td>0.375380</td>\n",
       "      <td>0.856339</td>\n",
       "      <td>0.143661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PM in New York</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579416</td>\n",
       "      <td>0.420584</td>\n",
       "      <td>0.904491</td>\n",
       "      <td>0.095509</td>\n",
       "      <td>0.995405</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.525655</td>\n",
       "      <td>0.474345</td>\n",
       "      <td>0.946647</td>\n",
       "      <td>0.053353</td>\n",
       "      <td>0.629131</td>\n",
       "      <td>0.370869</td>\n",
       "      <td>0.805269</td>\n",
       "      <td>0.194731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How Bout Now</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644894</td>\n",
       "      <td>0.355106</td>\n",
       "      <td>0.960787</td>\n",
       "      <td>0.039213</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.553012</td>\n",
       "      <td>0.446988</td>\n",
       "      <td>0.969510</td>\n",
       "      <td>0.030490</td>\n",
       "      <td>0.627895</td>\n",
       "      <td>0.372105</td>\n",
       "      <td>0.837856</td>\n",
       "      <td>0.162144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My Side</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.325871</td>\n",
       "      <td>0.977808</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>0.999302</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.571914</td>\n",
       "      <td>0.428086</td>\n",
       "      <td>0.967398</td>\n",
       "      <td>0.032602</td>\n",
       "      <td>0.629288</td>\n",
       "      <td>0.370712</td>\n",
       "      <td>0.858425</td>\n",
       "      <td>0.141575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 credits  log_drake  log_quen  sgd_drake  sgd_quen  svc_drake  \\\n",
       "Legend               1.0   0.597164  0.402836   0.626774  0.373226   0.907117   \n",
       "Energy               0.0   0.529922  0.470078   0.627841  0.372159   0.811741   \n",
       "10 Bands             1.0   0.455955  0.544045   0.431652  0.568348   0.084154   \n",
       "Know Yourself        1.0   0.550921  0.449079   0.613523  0.386477   0.849871   \n",
       "No Tellin’           0.0   0.471311  0.528689   0.468214  0.531786   0.120009   \n",
       "Madonna              0.0   0.571905  0.428095   0.719589  0.280411   0.952187   \n",
       "6 God                0.0   0.572521  0.427479   0.683747  0.316253   0.920352   \n",
       "Star67               0.0   0.536639  0.463361   0.703808  0.296192   0.929450   \n",
       "Preach               0.0   0.564851  0.435149   0.663840  0.336160   0.878252   \n",
       "Used To              1.0   0.576648  0.423352   0.765248  0.234752   0.975054   \n",
       "6 Man                0.0   0.568942  0.431058   0.711573  0.288427   0.946372   \n",
       "Now & Forever        0.0   0.588651  0.411349   0.643662  0.356338   0.842335   \n",
       "Company              0.0   0.667774  0.332226   0.995657  0.004343   0.999559   \n",
       "You & The 6          0.0   0.612233  0.387767   0.907056  0.092944   0.995669   \n",
       "Jungle               0.0   0.738207  0.261793   1.000000  0.000000   0.999863   \n",
       "6PM in New York      0.0   0.579416  0.420584   0.904491  0.095509   0.995405   \n",
       "How Bout Now         0.0   0.644894  0.355106   0.960787  0.039213   0.998936   \n",
       "My Side              0.0   0.674129  0.325871   0.977808  0.022192   0.999302   \n",
       "\n",
       "                 svc_quen  rfc_drake  rfc_quen  ada_drake  ada_quen  \\\n",
       "Legend           0.092883       0.67      0.33   0.541413  0.458587   \n",
       "Energy           0.188259       0.61      0.39   0.558265  0.441735   \n",
       "10 Bands         0.915846       0.56      0.44   0.473329  0.526671   \n",
       "Know Yourself    0.150129       0.65      0.35   0.614490  0.385510   \n",
       "No Tellin’       0.879991       0.63      0.37   0.494069  0.505931   \n",
       "Madonna          0.047813       0.74      0.26   0.557648  0.442352   \n",
       "6 God            0.079648       0.67      0.33   0.566147  0.433853   \n",
       "Star67           0.070550       0.65      0.35   0.494910  0.505090   \n",
       "Preach           0.121748       0.56      0.44   0.536151  0.463849   \n",
       "Used To          0.024946       0.76      0.24   0.514420  0.485580   \n",
       "6 Man            0.053628       0.62      0.38   0.514141  0.485859   \n",
       "Now & Forever    0.157665       0.67      0.33   0.612740  0.387260   \n",
       "Company          0.000441       0.72      0.28   0.596438  0.403562   \n",
       "You & The 6      0.004331       0.91      0.09   0.736355  0.263645   \n",
       "Jungle           0.000137       0.87      0.13   0.551691  0.448309   \n",
       "6PM in New York  0.004595       0.88      0.12   0.525655  0.474345   \n",
       "How Bout Now     0.001064       0.90      0.10   0.553012  0.446988   \n",
       "My Side          0.000698       0.96      0.04   0.571914  0.428086   \n",
       "\n",
       "                 gbc_drake  gbc_quen  cnn_drake  cnn_quen  ens_drake  ens_quen  \n",
       "Legend            0.953654  0.046346   0.622125  0.377875   0.716020  0.283980  \n",
       "Energy            0.855129  0.144871   0.628946  0.371054   0.665483  0.334517  \n",
       "10 Bands          0.204444  0.795556   0.626616  0.373384   0.368256  0.631744  \n",
       "Know Yourself     0.937369  0.062631   0.624701  0.375299   0.702696  0.297304  \n",
       "No Tellin’        0.366704  0.633296   0.624795  0.375205   0.425051  0.574949  \n",
       "Madonna           0.932966  0.067034   0.623011  0.376989   0.745716  0.254284  \n",
       "6 God             0.768669  0.231331   0.625225  0.374775   0.696906  0.303094  \n",
       "Star67            0.655004  0.344996   0.628706  0.371294   0.661635  0.338365  \n",
       "Preach            0.681359  0.318641   0.623281  0.376719   0.647409  0.352591  \n",
       "Used To           0.878076  0.121924   0.627075  0.372925   0.744908  0.255092  \n",
       "6 Man             0.931333  0.068667   0.625370  0.374630   0.715393  0.284607  \n",
       "Now & Forever     0.885292  0.114708   0.620968  0.379032   0.707113  0.292887  \n",
       "Company           0.942788  0.057212   0.626480  0.373520   0.820369  0.179631  \n",
       "You & The 6       0.987269  0.012731   0.625742  0.374258   0.858097  0.141903  \n",
       "Jungle            0.978274  0.021726   0.624620  0.375380   0.856339  0.143661  \n",
       "6PM in New York   0.946647  0.053353   0.629131  0.370869   0.805269  0.194731  \n",
       "How Bout Now      0.969510  0.030490   0.627895  0.372105   0.837856  0.162144  \n",
       "My Side           0.967398  0.032602   0.629288  0.370712   0.858425  0.141575  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iyrtitl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_drake</th>\n",
       "      <th>log_quen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Legend</th>\n",
       "      <td>0.597164</td>\n",
       "      <td>0.402836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>0.529922</td>\n",
       "      <td>0.470078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Bands</th>\n",
       "      <td>0.455955</td>\n",
       "      <td>0.544045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know Yourself</th>\n",
       "      <td>0.550921</td>\n",
       "      <td>0.449079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Tellin’</th>\n",
       "      <td>0.471311</td>\n",
       "      <td>0.528689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madonna</th>\n",
       "      <td>0.571905</td>\n",
       "      <td>0.428095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 God</th>\n",
       "      <td>0.572521</td>\n",
       "      <td>0.427479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star67</th>\n",
       "      <td>0.536639</td>\n",
       "      <td>0.463361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preach</th>\n",
       "      <td>0.564851</td>\n",
       "      <td>0.435149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Used To</th>\n",
       "      <td>0.576648</td>\n",
       "      <td>0.423352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Man</th>\n",
       "      <td>0.568942</td>\n",
       "      <td>0.431058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Now &amp; Forever</th>\n",
       "      <td>0.588651</td>\n",
       "      <td>0.411349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>0.667774</td>\n",
       "      <td>0.332226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You &amp; The 6</th>\n",
       "      <td>0.612233</td>\n",
       "      <td>0.387767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>0.738207</td>\n",
       "      <td>0.261793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PM in New York</th>\n",
       "      <td>0.579416</td>\n",
       "      <td>0.420584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How Bout Now</th>\n",
       "      <td>0.644894</td>\n",
       "      <td>0.355106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My Side</th>\n",
       "      <td>0.674129</td>\n",
       "      <td>0.325871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 log_drake  log_quen\n",
       "Legend            0.597164  0.402836\n",
       "Energy            0.529922  0.470078\n",
       "10 Bands          0.455955  0.544045\n",
       "Know Yourself     0.550921  0.449079\n",
       "No Tellin’        0.471311  0.528689\n",
       "Madonna           0.571905  0.428095\n",
       "6 God             0.572521  0.427479\n",
       "Star67            0.536639  0.463361\n",
       "Preach            0.564851  0.435149\n",
       "Used To           0.576648  0.423352\n",
       "6 Man             0.568942  0.431058\n",
       "Now & Forever     0.588651  0.411349\n",
       "Company           0.667774  0.332226\n",
       "You & The 6       0.612233  0.387767\n",
       "Jungle            0.738207  0.261793\n",
       "6PM in New York   0.579416  0.420584\n",
       "How Bout Now      0.644894  0.355106\n",
       "My Side           0.674129  0.325871"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model's prediction\n",
    "df_iyrtitl[[best_key+'_drake', best_key+ \"_quen\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(key, data): \n",
    "    data=model_dict[best_key]['vect'].transform(data)\n",
    "    data=model_dict[best_key]['tfidf'].transform(data)\n",
    "    return model_dict[best_key]['clf'].predict_proba(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(key, data):\n",
    "    data=model_dict[best_key]['vect'].transform(data)\n",
    "    data=model_dict[best_key]['tfidf'].transform(data)\n",
    "    return model_dict[best_key]['clf'].predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= predict_proba(best_key, X_test)\n",
    "# model_dict[best_key]['clf'].predict_proba(model_dict[best_key]['tfidf'].transform(model_dict[best_key]['vect'].transform(X_test)))\n",
    "test_predictions_df=pd.DataFrame(preds, index=test_titles, columns=['drake_prob', 'quen_prob'])\n",
    "test_predictions_df['prediction']=predict(best_key, X_test)\n",
    "test_predictions_df['true']=y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drake_prob</th>\n",
       "      <th>quen_prob</th>\n",
       "      <th>prediction</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dreams Money Can Buy</th>\n",
       "      <td>0.543780</td>\n",
       "      <td>0.456220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 AM in Toronto</th>\n",
       "      <td>0.549802</td>\n",
       "      <td>0.450198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heat of the Moment</th>\n",
       "      <td>0.676649</td>\n",
       "      <td>0.323351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zodiac Sign</th>\n",
       "      <td>0.616283</td>\n",
       "      <td>0.383717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intro</th>\n",
       "      <td>0.556434</td>\n",
       "      <td>0.443566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Landed</th>\n",
       "      <td>0.492981</td>\n",
       "      <td>0.507019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4L</th>\n",
       "      <td>0.446813</td>\n",
       "      <td>0.553187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do What You Do (Remix)</th>\n",
       "      <td>0.651885</td>\n",
       "      <td>0.348115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sacrifices</th>\n",
       "      <td>0.459351</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Can’t Have Everything</th>\n",
       "      <td>0.527941</td>\n",
       "      <td>0.472059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305 to My City</th>\n",
       "      <td>0.504053</td>\n",
       "      <td>0.495947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intro (Room For Improvement)</th>\n",
       "      <td>0.677202</td>\n",
       "      <td>0.322798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do What You Do</th>\n",
       "      <td>0.613459</td>\n",
       "      <td>0.386541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drake’s Voice Mail Box #2</th>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>About the Game</th>\n",
       "      <td>0.592804</td>\n",
       "      <td>0.407196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Try Harder</th>\n",
       "      <td>0.623545</td>\n",
       "      <td>0.376455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diplomatic Immunity</th>\n",
       "      <td>0.565685</td>\n",
       "      <td>0.434315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I’m Upset</th>\n",
       "      <td>0.516441</td>\n",
       "      <td>0.483559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Fantasy</th>\n",
       "      <td>0.646629</td>\n",
       "      <td>0.353371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Successful</th>\n",
       "      <td>0.611730</td>\n",
       "      <td>0.388270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best I Ever Had</th>\n",
       "      <td>0.586408</td>\n",
       "      <td>0.413592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I’m Goin In</th>\n",
       "      <td>0.556984</td>\n",
       "      <td>0.443016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Calm</th>\n",
       "      <td>0.596611</td>\n",
       "      <td>0.403389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fear</th>\n",
       "      <td>0.590564</td>\n",
       "      <td>0.409436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Let’s Call It Off</th>\n",
       "      <td>0.627164</td>\n",
       "      <td>0.372836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Best I Ever Had</th>\n",
       "      <td>0.586408</td>\n",
       "      <td>0.413592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Congratulations</th>\n",
       "      <td>0.573381</td>\n",
       "      <td>0.426619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headlines</th>\n",
       "      <td>0.556096</td>\n",
       "      <td>0.443904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Take Care</th>\n",
       "      <td>0.660432</td>\n",
       "      <td>0.339568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under Ground Kings</th>\n",
       "      <td>0.582392</td>\n",
       "      <td>0.417608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karaoke</th>\n",
       "      <td>0.598644</td>\n",
       "      <td>0.401356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Find Your Love</th>\n",
       "      <td>0.535882</td>\n",
       "      <td>0.464118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Too Good (Demo)</th>\n",
       "      <td>0.663958</td>\n",
       "      <td>0.336042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not Nice</th>\n",
       "      <td>0.808311</td>\n",
       "      <td>0.191689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keep the Family Close</th>\n",
       "      <td>0.609392</td>\n",
       "      <td>0.390608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hype</th>\n",
       "      <td>0.546438</td>\n",
       "      <td>0.453562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Controlla</th>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4:21 Freestyle</th>\n",
       "      <td>0.450566</td>\n",
       "      <td>0.549434</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug28...</th>\n",
       "      <td>0.467452</td>\n",
       "      <td>0.532548</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bank!!!</th>\n",
       "      <td>0.342006</td>\n",
       "      <td>0.657994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cease and Desist</th>\n",
       "      <td>0.500250</td>\n",
       "      <td>0.499750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Criss Angel</th>\n",
       "      <td>0.370260</td>\n",
       "      <td>0.629740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eclipse</th>\n",
       "      <td>0.536149</td>\n",
       "      <td>0.463851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fax</th>\n",
       "      <td>0.428446</td>\n",
       "      <td>0.571554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Games Freestyle... (No Diss)</th>\n",
       "      <td>0.392060</td>\n",
       "      <td>0.607940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Going Back</th>\n",
       "      <td>0.332724</td>\n",
       "      <td>0.667276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grey Leather</th>\n",
       "      <td>0.440153</td>\n",
       "      <td>0.559847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intended</th>\n",
       "      <td>0.502046</td>\n",
       "      <td>0.497954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaw Droppin...</th>\n",
       "      <td>0.429928</td>\n",
       "      <td>0.570072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Love Costs.</th>\n",
       "      <td>0.510348</td>\n",
       "      <td>0.489652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Post Malone</th>\n",
       "      <td>0.450769</td>\n",
       "      <td>0.549231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power 106 Freestyle with Quentin Miller</th>\n",
       "      <td>0.475550</td>\n",
       "      <td>0.524450</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Praying Mantis Freestyle</th>\n",
       "      <td>0.488508</td>\n",
       "      <td>0.511492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senseless.</th>\n",
       "      <td>0.485321</td>\n",
       "      <td>0.514679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Values...</th>\n",
       "      <td>0.411539</td>\n",
       "      <td>0.588461</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         drake_prob  quen_prob  prediction  \\\n",
       "Dreams Money Can Buy                       0.543780   0.456220         0.0   \n",
       "5 AM in Toronto                            0.549802   0.450198         0.0   \n",
       "Heat of the Moment                         0.676649   0.323351         0.0   \n",
       "Zodiac Sign                                0.616283   0.383717         0.0   \n",
       "Intro                                      0.556434   0.443566         0.0   \n",
       "Landed                                     0.492981   0.507019         1.0   \n",
       "D4L                                        0.446813   0.553187         1.0   \n",
       "Do What You Do (Remix)                     0.651885   0.348115         0.0   \n",
       "Sacrifices                                 0.459351   0.540649         1.0   \n",
       "Can’t Have Everything                      0.527941   0.472059         0.0   \n",
       "305 to My City                             0.504053   0.495947         0.0   \n",
       "Intro (Room For Improvement)               0.677202   0.322798         0.0   \n",
       "Do What You Do                             0.613459   0.386541         0.0   \n",
       "Drake’s Voice Mail Box #2                  0.602500   0.397500         0.0   \n",
       "About the Game                             0.592804   0.407196         0.0   \n",
       "Try Harder                                 0.623545   0.376455         0.0   \n",
       "Diplomatic Immunity                        0.565685   0.434315         0.0   \n",
       "I’m Upset                                  0.516441   0.483559         0.0   \n",
       "Final Fantasy                              0.646629   0.353371         0.0   \n",
       "Successful                                 0.611730   0.388270         0.0   \n",
       "Best I Ever Had                            0.586408   0.413592         0.0   \n",
       "I’m Goin In                                0.556984   0.443016         0.0   \n",
       "The Calm                                   0.596611   0.403389         0.0   \n",
       "Fear                                       0.590564   0.409436         0.0   \n",
       "Let’s Call It Off                          0.627164   0.372836         0.0   \n",
       "Best I Ever Had                            0.586408   0.413592         0.0   \n",
       "Congratulations                            0.573381   0.426619         0.0   \n",
       "Headlines                                  0.556096   0.443904         0.0   \n",
       "Take Care                                  0.660432   0.339568         0.0   \n",
       "Under Ground Kings                         0.582392   0.417608         0.0   \n",
       "Karaoke                                    0.598644   0.401356         0.0   \n",
       "Find Your Love                             0.535882   0.464118         0.0   \n",
       "Too Good (Demo)                            0.663958   0.336042         0.0   \n",
       "Not Nice                                   0.808311   0.191689         0.0   \n",
       "Keep the Family Close                      0.609392   0.390608         0.0   \n",
       "Hype                                       0.546438   0.453562         0.0   \n",
       "Controlla                                  0.627119   0.372881         0.0   \n",
       "4:21 Freestyle                             0.450566   0.549434         1.0   \n",
       "Aug28...                                   0.467452   0.532548         1.0   \n",
       "Bank!!!                                    0.342006   0.657994         1.0   \n",
       "Cease and Desist                           0.500250   0.499750         0.0   \n",
       "Criss Angel                                0.370260   0.629740         1.0   \n",
       "Eclipse                                    0.536149   0.463851         0.0   \n",
       "Fax                                        0.428446   0.571554         1.0   \n",
       "Games Freestyle... (No Diss)               0.392060   0.607940         1.0   \n",
       "Going Back                                 0.332724   0.667276         1.0   \n",
       "Grey Leather                               0.440153   0.559847         1.0   \n",
       "Intended                                   0.502046   0.497954         0.0   \n",
       "Jaw Droppin...                             0.429928   0.570072         1.0   \n",
       "Love Costs.                                0.510348   0.489652         0.0   \n",
       "Post Malone                                0.450769   0.549231         1.0   \n",
       "Power 106 Freestyle with Quentin Miller    0.475550   0.524450         1.0   \n",
       "Praying Mantis Freestyle                   0.488508   0.511492         1.0   \n",
       "Senseless.                                 0.485321   0.514679         1.0   \n",
       "Values...                                  0.411539   0.588461         1.0   \n",
       "\n",
       "                                         true  \n",
       "Dreams Money Can Buy                      0.0  \n",
       "5 AM in Toronto                           0.0  \n",
       "Heat of the Moment                        0.0  \n",
       "Zodiac Sign                               0.0  \n",
       "Intro                                     0.0  \n",
       "Landed                                    0.0  \n",
       "D4L                                       0.0  \n",
       "Do What You Do (Remix)                    0.0  \n",
       "Sacrifices                                0.0  \n",
       "Can’t Have Everything                     0.0  \n",
       "305 to My City                            0.0  \n",
       "Intro (Room For Improvement)              0.0  \n",
       "Do What You Do                            0.0  \n",
       "Drake’s Voice Mail Box #2                 0.0  \n",
       "About the Game                            0.0  \n",
       "Try Harder                                0.0  \n",
       "Diplomatic Immunity                       0.0  \n",
       "I’m Upset                                 0.0  \n",
       "Final Fantasy                             0.0  \n",
       "Successful                                0.0  \n",
       "Best I Ever Had                           0.0  \n",
       "I’m Goin In                               0.0  \n",
       "The Calm                                  0.0  \n",
       "Fear                                      0.0  \n",
       "Let’s Call It Off                         0.0  \n",
       "Best I Ever Had                           0.0  \n",
       "Congratulations                           0.0  \n",
       "Headlines                                 0.0  \n",
       "Take Care                                 0.0  \n",
       "Under Ground Kings                        0.0  \n",
       "Karaoke                                   0.0  \n",
       "Find Your Love                            0.0  \n",
       "Too Good (Demo)                           0.0  \n",
       "Not Nice                                  0.0  \n",
       "Keep the Family Close                     0.0  \n",
       "Hype                                      0.0  \n",
       "Controlla                                 0.0  \n",
       "4:21 Freestyle                            1.0  \n",
       "Aug28...                                  1.0  \n",
       "Bank!!!                                   1.0  \n",
       "Cease and Desist                          1.0  \n",
       "Criss Angel                               1.0  \n",
       "Eclipse                                   1.0  \n",
       "Fax                                       1.0  \n",
       "Games Freestyle... (No Diss)              1.0  \n",
       "Going Back                                1.0  \n",
       "Grey Leather                              1.0  \n",
       "Intended                                  1.0  \n",
       "Jaw Droppin...                            1.0  \n",
       "Love Costs.                               1.0  \n",
       "Post Malone                               1.0  \n",
       "Power 106 Freestyle with Quentin Miller   1.0  \n",
       "Praying Mantis Freestyle                  1.0  \n",
       "Senseless.                                1.0  \n",
       "Values...                                 1.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track words that are best \"drake\" predictors, \"miller\" predictors, and generally influenctial\n",
    "drake_tokens=[]\n",
    "quentin_tokens=[]\n",
    "total_tokens=[]\n",
    "\n",
    "# keys for models that have meaningful coefficients\n",
    "keys1 = ['log', 'sgd', 'svc']\n",
    "\n",
    "\n",
    "for key in keys1: \n",
    "    cdf = pd.DataFrame(model_dict[key]['clf'].coef_.T, \n",
    "                   model_dict[key]['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "    drake_tokens.append(cdf.index.tolist()[:10])\n",
    "    total_tokens.append(cdf.index.tolist()[:10])\n",
    "    total_tokens.append(cdf.index.tolist()[-10:])\n",
    "    quentin_tokens.append(cdf.index.tolist()[-10:])\n",
    "\n",
    "#keys for models with features that have magnitude but no direction\n",
    "keys2 = ['rfc', 'ada', 'gbc']\n",
    "for key in keys2: \n",
    "    cdf = pd.DataFrame(model_dict[key]['clf'].feature_importances_.T, \n",
    "                   model_dict[key]['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "    total_tokens.append(cdf[-20:].index.tolist())\n",
    "l_drake=functools.reduce(lambda x, y : x+y, drake_tokens, [])\n",
    "l_quen=functools.reduce(lambda x, y : x+y, quentin_tokens, [])\n",
    "l_tot=functools.reduce(lambda x, y : x+y, total_tokens, [])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "t=Counter(l_tot)\n",
    "plt.bar(t.keys(), [x/6 for x in t.values()])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel(\"Percentage of models where word was influential\")\n",
    "plt.title(\"Most influential predictors\")\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top drake predictors\n",
      "girl\n",
      "re\n",
      "oh\n",
      "oh oh\n",
      "you re\n",
      "caus\n",
      "love\n",
      "are\n",
      "down\n",
      "babi\n",
      "fade fade\n",
      "thi is\n",
      "yo\n",
      "octob\n",
      "night\n",
      "\n",
      "Top Quentin Miller predictors\n",
      "made\n",
      "beat\n",
      "gon\n",
      "wait wait\n",
      "came in\n",
      "nike\n",
      "yuh\n",
      "wait\n",
      "came\n",
      "yeah yeah\n",
      "my daughter\n",
      "daughter\n",
      "live\n",
      "1317\n",
      "hit\n",
      "blue\n"
     ]
    }
   ],
   "source": [
    "print(\"Top drake predictors\")\n",
    "[print(x) for x in Counter(l_drake)]\n",
    "print(\"\\nTop Quentin Miller predictors\")\n",
    "[print(x) for x in Counter(l_quen)];\n",
    "# print(\"Generally influential predictors\")\n",
    "# [print(x) for x in total_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Drake Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up memory to make sure there's space for reorganized corpus\n",
    "del X_train\n",
    "del y_train \n",
    "\n",
    "#reorganize corpus by album for further exploration of which songs are most \"Drake-like\"\n",
    "album_dict={}\n",
    "song_titles_dict={}\n",
    "for album in drake: \n",
    "    if len(album) < 10:\n",
    "        continue\n",
    "    album_dict[album[0]['album'].strip()]= [stop_removal(song['lyrics']) for song in album]\n",
    "    song_titles_dict[album[0]['album'].strip()] = [song['title'] for song in album]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'album_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-936f27f3cdbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malbum_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malbum_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'album_key' is not defined"
     ]
    }
   ],
   "source": [
    "len(album_dict[album_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_mean={}\n",
    "for album_key in album_dict:\n",
    "    X_train= model_dict[best_key]['tfidf'].transform(model_dict[best_key]['vect'].transform(album_dict[album_key]))\n",
    "    preds=model_dict[best_key]['clf'].predict_proba(X_train)[:,0]\n",
    "    album_mean[album_key]=np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(album_mean.keys(), album_mean.values())\n",
    "plt.xticks(range(len(album_mean)), album_mean.keys(), rotation='vertical')\n",
    "plt.ylim(0.5,1)\n",
    "plt.ylabel(\"Mean probability album's songs are by 'Drake'\")\n",
    "plt.title(\"'Drakeness' of Drake albums based on 7 models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work on the Drake-Miller Controversy\n",
    "\n",
    "- <ins>Cleaning the data</ins>: some of the songs that were misclassified were 'outlier' songs in terms of being abnormally short or non-traditional (for example, a spoken word intro). These songs could be removed in future iterations to improve classification accuracy. Other preprocessing choices are available such as lemmatization over stemming or more aggressive removal of stop words.   \n",
    "- <ins>Supplementing the data </ins>: The relative easiness of the binary classification problem limits how optimistic we can be about the results on the piece of ambiguous authorship. Even though our models suggest that portions of iyrtitl were more likely to be written by Quentin Miller than other Drake songs, it could well be that the album was simply differnt than other Drake projects and not actually closer to Miller's style. It could be that the album was closer to some other artist Drake was trying to imitate at the time. Future attempts might add 'noise' to the task by including other artists and turning it into a multi-class problem to see if iyrtitl's relative closeness to Miller's style still bears out. The data folder of this project already has albums from J. Cole -[an artist found to be close to Drake](https://pudding.cool/2017/09/hip-hop-words/)- and PartyNextDoor - a frequent collaborator who has also received writing credits on Drake songs. \n",
    "- <ins>Changing document size</ins>: So far, we've analyzed songs as documents. A different approach might scale upwards by looking at whole albums as a single document or scale downards by looking at individual verses, bars, or lines as documents. The latter few approaches could be interesting by trying to go beyond the bag of words or even basic word-embedding approach by attempting to learn rhyme scheme, assonance, consonance, and meter.\n",
    "- <ins>Additional models</ins> Beyond improving the existing models with better hyperparameters, other classification strategies could provide further insight into the data. Based on the success of linear modeling, perhaps LDA (or even QDA) would also be a solid classifier. Other unsupervised methods (PCA, Clustering), could offer insight into similarities and better visualizations of the distance between individual songs and albums.\n",
    "- <ins>Full stacking algorithm</ins>: The final model presented is a naive ensemble method that merely aggregates predictions of the 7 other models. There could be some advantages to this approach compared to simply picking a single model based on validation accuracy because of the disparate preprocessing procedures. At present, there was a small positive effect on test set performance which indicates there is some potential to the idea. One easy step to improve the current aggregate approach would be weighting sub-models based on their performance metrics. The most empircally robust approach, though, would be to implement a full stacking algorithm where we deploy models based on their effectiveness on subsets of the feature space. Stacking is generally conducted with weaker, more independent learners than those used in the present appraoch. Thus, stacking would require a substantial rework of the base learners. However, considering the size and sparsity of the feature space, it is very possible some models do better than others in certain cases, meaning there could be additional performance gains to be had through stacking. \n",
    "- <ins>Statistical approach</ins>: This method took a more machine learning-centric approach in optimizing on classification accuracy. We made relatively few assumpsions about the data by trying all sorts of differnt models. At present, each model had unique preprocessing. This choice limits the efficacy of our interpretive efforts and our ability to compare models beyond classification performance. For instance, the lack of standard scaling means that we really can't compare the magnitude of coefficients across models to determine which words are most influential beyond the fuzzy \"15 most influential features\" approach employed here. Now that we've found linear models to perform well, we could hone in on these methods and make further assumpsions to gain insight into how each author uses contextual and non-contextual words differently. \n",
    "- <ins>Further nueral network exploration</ins>: The current convolutional nueral network is a very simple model, in part due to limitations in processing power. Beyond experimentation with different architectures or hyperparameters, we could also try using pretrained word embeddings (eg GloVe, Fasttext, Word2Vec) to get better predictions while also keeping training time manageable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Applications\n",
    "- <ins>Ghostwriting in hip-hop</ins>: So far we've looked at the most famous example of ghostwriting in hip hop. This approach could be applied to rumored ghostwriting connections (see basically any Dr. Dre or P. Diddy song). It could also be applied to credited writers to see how much a song is a collaborative affair vs. a case of the credited writer doing most of the lifting with the artist just taking credit for it. The most ambitious application, however, would be a 'cold start' approach where we have to look for ghostwriting or plagiarism without knowing in advance who is the most likely ghost writer. For instance, what if Miller had never been outed? Could an algorithm have identified Miller out of a broad set of artists as the real author of \"10 Bands?\" As mentioned above, the multiclass problem increases the complexity immensely, so at present it seems unlikely 'cold start' identifications are possible. Probably somewhere in the middle - where a set of potential writers are identified in advance and then machine learning tries to find stylistic similarity/influence  - is the most promising area for future work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
