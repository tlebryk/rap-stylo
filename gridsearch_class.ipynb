{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from time import time\n",
    "# import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from nltk import FreqDist\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
    "from keras.optimizers import SGD \n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes path, returns list with of albums in json format\n",
    "# each json entry is a song. \n",
    "def json_extract(path):\n",
    "    data_list=[]\n",
    "    for file in os.listdir(path): \n",
    "        if file[-5:] == '.json':\n",
    "            with open(path+file, 'r') as f: \n",
    "                data = json.load(f)\n",
    "                data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/drake/'\n",
    "drake=json_extract(path)\n",
    "path='data/quentin_miller/'\n",
    "quentin=json_extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_count=0\n",
    "q_count=0\n",
    "iyrtitl=[]\n",
    "iyrtitl_set=[]\n",
    "for album in drake: \n",
    "    for song in album: \n",
    "        if song[\"album\"]==\"If You’re Reading This It’s Too Late \":\n",
    "            iyrtitl.append(song['title'])\n",
    "            iyrtitl_set.append(song['lyrics'])\n",
    "        else: \n",
    "            d_count +=1\n",
    "\n",
    "for album in quentin:\n",
    "    for song in album:\n",
    "        q_count +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 138 380\n"
     ]
    }
   ],
   "source": [
    "print(d_count, q_count, d_count+q_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data about each document in a seperate dictionary with keys as titles and and value containing title, album, and artist info. \n",
    "# helpful for checking dataframe of corpus and test set against album info. \n",
    "class Doc_info: \n",
    "    def __init__(self, title: str, album : str, artist : str):\n",
    "        self.title = title\n",
    "        self.album = album\n",
    "        self.artist = artist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "corpus = []\n",
    "iyrtitl= []\n",
    "test_set = []\n",
    "iyrtitl_set=[]\n",
    "corpus_doc_ls = []\n",
    "test_doc_ls = []\n",
    "d_test_cnt = 0\n",
    "d_corp_cnt=0\n",
    "\n",
    "for album in drake: \n",
    "    for song in album: \n",
    "        # keep track of \"If you're...\" to put in a separate set (iytitl is our ambiguous authorship)\n",
    "        if song[\"album\"]==\"If You’re Reading This It’s Too Late \":\n",
    "            iyrtitl.append(song['title'])\n",
    "            iyrtitl_set.append(song['lyrics'])\n",
    "        else: \n",
    "            # place more drake songs in test set to better balance training sample\n",
    "            if np.random.rand(1) < 0.15: \n",
    "                test_set.append(song['lyrics'])\n",
    "                test_doc_ls.append(Doc_info(song['title'], song['album'], 'drake'))\n",
    "                d_test_cnt+=1\n",
    "            else: \n",
    "                corpus.append(song['lyrics'])\n",
    "                corpus_doc_ls.append(Doc_info(song['title'], song['album'], 'drake'))\n",
    "                d_corp_cnt+=1\n",
    "                \n",
    "for album in quentin: \n",
    "    for song in album: \n",
    "        if np.random.rand(1) < 0.1: \n",
    "            test_set.append(song['lyrics'])\n",
    "            test_doc_ls.append(Doc_info(song['title'], song['album'], 'quentin'))\n",
    "        else: \n",
    "            corpus.append(song['lyrics'])\n",
    "            corpus_doc_ls.append(Doc_info(song['title'], song['album'], 'quentin'))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(len(corpus))\n",
    "y_train[d_corp_cnt:] =1\n",
    "y_test = np.zeros(len(test_set))\n",
    "y_test[d_test_cnt:] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"quentin songs found:\", len(quentin))\n",
    "print(\"drake songs found:\", len(drake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops used to remove obvious predictive words.\n",
    "stops=['drizzy', 'drake', 'quentin', 'miller', 'ovo', 'champagne', 'papi','toronto', 'atlanta', '6']\n",
    "#used to get rid of common non-contexual words to focus which contextual words are most influential. \n",
    "stops2=stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Legend</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Bands</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know Yourself</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Tellin’</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madonna</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 God</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star67</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preach</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Used To</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Man</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Now &amp; Forever</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You &amp; The 6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PM in New York</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How Bout Now</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My Side</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 credits\n",
       "Legend               1.0\n",
       "Energy               0.0\n",
       "10 Bands             1.0\n",
       "Know Yourself        1.0\n",
       "No Tellin’           0.0\n",
       "Madonna              0.0\n",
       "6 God                0.0\n",
       "Star67               0.0\n",
       "Preach               0.0\n",
       "Used To              1.0\n",
       "6 Man                0.0\n",
       "Now & Forever        0.0\n",
       "Company              0.0\n",
       "You & The 6          0.0\n",
       "Jungle               0.0\n",
       "6PM in New York      0.0\n",
       "How Bout Now         0.0\n",
       "My Side              0.0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep track of accuracy score of all 6 models \n",
    "score_dict = {}\n",
    "# track words that are best \"drake\" predictors, \"miller\" predictors, and general predictors\n",
    "drake_tokens=[]\n",
    "quentin_tokens=[]\n",
    "total_tokens=[]\n",
    "# track the test set predictions for held out if you're reading this its too late \n",
    "df=pd.DataFrame(np.zeros(len(iyrtitl)), index=iyrtitl, columns=[\"credits\"])\n",
    "credits = ['10 Bands', \"Legend\", \"Know Yourself\", \"Used To\"]\n",
    "for name in credits:\n",
    "    df.loc[name]=1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don\\'t have a fuck to give, I\\'ve been moving state to state\\nIn my leather and my Timbs like it\\'s 1998\\nAnd my dog Chubby Chub, that\\'s my nigga from the way\\nOn the Eastside of the city, that\\'s where everybody stay\\nSeem like everybody calling \\'cause they want me on their song\\nIt\\'s like every time I touch it I could never do no wrong\\nWhen they need a favor from you, man, they don\\'t leave you alone\\nBut I guess that\\'s just the motion, yeah\\nYeah, looking back on it, at least my pride is intact\\n\\'Cause we said \"no strings attached\" and I still got tied up in that\\nEverything that I write is either for her or about her\\nSo I\\'m with her even when I\\'m here without her and she know it\\nThe girl that I wanna save is like a danger to my health\\nTry being with somebody that wanna be somebody else\\nI always thought she was perfect when she was being herself\\nDon\\'t even know how to help, but I guess that\\'s just the motion, yeah\\n'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf= TfidfTransformer(use_idf=True, norm='l1')\n",
    "# c=[\"one four two three four four four\", \"four five five seven\", \"one two three four five six seven eight nine\"]\n",
    "# v1 = cv.fit_transform(c)\n",
    "# d=pd.DataFrame(v1.toarray(), columns=cv.get_feature_names(), index = ['count0', 'count1', 'count2'])\n",
    "# v2=tf.fit_transform(v1.toarray())\n",
    "# d=d.append(pd.DataFrame(v2.toarray(), columns=cv.get_feature_names(),  index = ['tf0', 'tf1', 'tf2']))\n",
    "# d\n",
    "# # np.max(v1.toarray() - v2.toarray())\n",
    "# # np.max(v1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = CV().build_analyzer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(doc): \n",
    "    return (ps.stem(w) for w in analyze(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a pipeline, hyperparameters, and a number of folds. \n",
    "# prints information about the grid search and returns the GridSearchCV object with the best model\n",
    "def grid_search(pipeline, param, k):\n",
    "    grid = GridSearchCV(pipeline, param, cv=k, n_jobs=4, verbose =1)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\", param)\n",
    "    start = time()\n",
    "    grid.fit(corpus, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - start))\n",
    "    print(\"Best score: %0.3f\" % grid.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    for param_name in sorted(param.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, grid.best_params_[param_name]))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', penalty='elasticnet', solver='saga'))\n",
    "])\n",
    "\n",
    "param= {\n",
    "#     'vect__max_df': (.4, .5), \n",
    "#     'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'vect__stop_words': (stops, stops2),/\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__l1_ratio': (0.1, 0,5, 0.9, 0.95),\n",
    "    'clf__C' : (0.01, 0.1, 1., 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__ngram_range': ((1, 1), (1, 2)), 'clf__l1_ratio': (0.1, 0, 5, 0.9, 0.95), 'clf__C': (0.01, 0.1, 1.0, 10)}\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=4)]: Done 120 out of 120 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 128.407s\n",
      "Best score: 0.842\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tclf__l1_ratio: 0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, param, 3)\n",
    "best_estimator=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-211-ce3e14948608>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[0mscore_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-205-3a3fa81f723e>\u001b[0m in \u001b[0;36mstemming\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "best_estimator.score(best_estimator.predict(test_set), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['log']=grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['log_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['log_quen']=best_estimator['clf'].predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "      <th>log_drake</th>\n",
       "      <th>log_quen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Legend</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.577247</td>\n",
       "      <td>0.422753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504775</td>\n",
       "      <td>0.495225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Bands</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500850</td>\n",
       "      <td>0.499150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know Yourself</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.517119</td>\n",
       "      <td>0.482881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Tellin’</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503975</td>\n",
       "      <td>0.496025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madonna</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528818</td>\n",
       "      <td>0.471182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 God</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540827</td>\n",
       "      <td>0.459173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star67</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527664</td>\n",
       "      <td>0.472336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preach</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544037</td>\n",
       "      <td>0.455963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Used To</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528538</td>\n",
       "      <td>0.471462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Man</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552355</td>\n",
       "      <td>0.447645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Now &amp; Forever</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534216</td>\n",
       "      <td>0.465784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576234</td>\n",
       "      <td>0.423766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You &amp; The 6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546882</td>\n",
       "      <td>0.453118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600831</td>\n",
       "      <td>0.399169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PM in New York</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549797</td>\n",
       "      <td>0.450203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How Bout Now</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547956</td>\n",
       "      <td>0.452044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My Side</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581681</td>\n",
       "      <td>0.418319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 credits  log_drake  log_quen\n",
       "Legend               1.0   0.577247  0.422753\n",
       "Energy               0.0   0.504775  0.495225\n",
       "10 Bands             1.0   0.500850  0.499150\n",
       "Know Yourself        1.0   0.517119  0.482881\n",
       "No Tellin’           0.0   0.503975  0.496025\n",
       "Madonna              0.0   0.528818  0.471182\n",
       "6 God                0.0   0.540827  0.459173\n",
       "Star67               0.0   0.527664  0.472336\n",
       "Preach               0.0   0.544037  0.455963\n",
       "Used To              1.0   0.528538  0.471462\n",
       "6 Man                0.0   0.552355  0.447645\n",
       "Now & Forever        0.0   0.534216  0.465784\n",
       "Company              0.0   0.576234  0.423766\n",
       "You & The 6          0.0   0.546882  0.453118\n",
       "Jungle               0.0   0.600831  0.399169\n",
       "6PM in New York      0.0   0.549797  0.450203\n",
       "How Bout Now         0.0   0.547956  0.452044\n",
       "My Side              0.0   0.581681  0.418319"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].coef_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "drake_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[-15:])\n",
    "quentin_tokens.append(cdf.index.tolist()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    # every iteration has yielded tfidf-l2 as the best for prediction. \n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', SGDClassifier(class_weight='balanced', loss= 'modified_huber'))\n",
    "])\n",
    "\n",
    "params = {    \n",
    "#     'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "#     'clf__loss' : ('perceptron'),\n",
    "    \"clf__alpha\" : (0.0001, 0.001, 0.01),\n",
    "#     'clf__l1_ratio' : (0, 0.5, 0.9, 1),\n",
    "#     'clf__tol' : (0.0001, 0.001),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__ngram_range': ((1, 1), (1, 2)), 'clf__alpha': (0.0001, 0.001, 0.01)}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  18 out of  18 | elapsed:   15.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.937s\n",
      "Best score: 0.853\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, params, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator= grid.best_estimator_\n",
    "score_dict['sgd']=grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['sgd_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['sgd_quen']=best_estimator['clf'].predict_proba(test)[:,1]\n",
    "# best_evaluator['clf'].predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].coef_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "drake_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[-15:])\n",
    "quentin_tokens.append(cdf.index.tolist()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    # every iteration has yielded tfidf-l2 as the best for prediction. \n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', SVC(kernel='linear', class_weight='balanced', probability=True))\n",
    "])\n",
    "\n",
    "params = {    \n",
    "#     'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "#     'clf__kernel' : ('linear', 'poly', 'rbf'),\n",
    "    'clf__C' : (5, 10, 20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__ngram_range': ((1, 1), (1, 2)), 'clf__C': (5, 10, 20)}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  18 out of  18 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.156s\n",
      "Best score: 0.843\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, params, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['svc']=grid.best_score_\n",
    "best_estimator= grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['svc_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['svc_quen']=best_estimator['clf'].predict_proba(test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].coef_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "drake_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[:15])\n",
    "total_tokens.append(cdf.index.tolist()[-15:])\n",
    "quentin_tokens.append(cdf.index.tolist()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "params= {\n",
    "#     'vect__max_df': (.4, .5), \n",
    "    'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "# #     'vect__stop_words': (stops, stops2),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "#     'clf__n_estimators' : (50, 100, 125) ,\n",
    "    'clf__max_depth' : (30, 50, 100),\n",
    "#     'clf__max_leaf_nodes' : (5, 25, 50),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__min_df': (3, 5, 7), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__max_depth': (30, 50, 100)}\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:   40.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 42.064s\n",
      "Best score: 0.817\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 30\n",
      "\tvect__min_df: 5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, params, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['rdf']=grid.best_score_\n",
    "best_estimator = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['rdf_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['rdf_quen']=best_estimator['clf'].predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].feature_importances_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "total_tokens.append(cdf[-30:].index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf['Coefficients'][:30].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 most important features for splitting\n",
    "total_tokens.append(cdf[-30:].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {    \n",
    "#     'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__n_estimators' : (80, 100, 120),\n",
    "    'clf__learning_rate' : (0.1, 0.3, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__ngram_range': ((1, 1), (1, 2)), 'clf__n_estimators': (80, 100, 120), 'clf__learning_rate': (0.1, 0.3, 0.5)}\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:   48.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 50.250s\n",
      "Best score: 0.843\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.1\n",
      "\tclf__n_estimators: 120\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, params, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['ada']=grid.best_score_\n",
    "best_estimator= grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['ada_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['ada_quen']=best_estimator['clf'].predict_proba(test)[:,1]\n",
    "# best_evaluator['clf'].predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].feature_importances_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "# 30 most important features for splitting\n",
    "total_tokens.append(cdf[-30:].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('vect', CV(analyzer=stemming, stop_words=stops, max_df=0.4)),\n",
    "    # every iteration has yielded tfidf-l2 as the best for prediction. \n",
    "    ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),\n",
    "    ('clf', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {    \n",
    "#     'vect__min_df': (3, 5, 7), \n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__learning_rate' : (0.1,0.2, 0.3),\n",
    "#     'clf__n_estimators' : (50, 75, 100),\n",
    "#     'clf__min_samples_leaf' : (1,2,5),\n",
    "    'clf__max_depth' : (2,3,5),\n",
    "#     'clf__min_impurity_decrease' : (0, 0.01, 0.05),\n",
    "#     'clf__tol' = (0.0001, 0.001, 0.005),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters: {'vect__ngram_range': ((1, 1), (1, 2)), 'clf__learning_rate': (0.1, 0.2, 0.3), 'clf__max_depth': (2, 3, 5)}\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=4)]: Done  54 out of  54 | elapsed:   56.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 58.851s\n",
      "Best score: 0.840\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.3\n",
      "\tclf__max_depth: 2\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "grid=grid_search(pipeline, params, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict['gdb']=grid.best_score_\n",
    "best_estimator= grid.best_estimator_\n",
    "test=best_estimator['vect'].transform(iyrtitl_set)\n",
    "test=best_estimator['tfidf'].transform(test)\n",
    "df['gdb_drake']=best_estimator['clf'].predict_proba(test)[:,0]\n",
    "df['gdb_quen']=best_estimator['clf'].predict_proba(test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(best_estimator['clf'].feature_importances_.T, \n",
    "                   best_estimator['vect'].get_feature_names(), \n",
    "                   columns=['Coefficients']).sort_values(['Coefficients'])\n",
    "\n",
    "# 15 most important features for splitting\n",
    "total_tokens.append(cdf[-30:].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CV(analyzer=stemming, stop_words=stops, min_df = 5, max_df=0.4)\n",
    "    # every iteration has yielded tfidf-l2 as the best for prediction. \n",
    "tf=TfidfTransformer(use_idf=True, norm='l1')\n",
    "r=tf.fit_transform(cv.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 10)                12900     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 30)                630       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 14,391\n",
      "Trainable params: 14,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(r.shape[1])\n",
    "#get proper x_train shape\n",
    "model = Sequential([\n",
    "    Dense(10, input_dim=r.shape[1], activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(30, activation='relu'),\n",
    "    Dense(20, activation='relu'),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                              patience=10, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 90 samples\n",
      "Epoch 1/500\n",
      "210/210 [==============================] - 0s 57us/step - loss: 1.4084e-05 - acc: 1.0000 - val_loss: 6.5290 - val_acc: 0.1667\n",
      "Epoch 2/500\n",
      "210/210 [==============================] - 0s 47us/step - loss: 1.3706e-05 - acc: 1.0000 - val_loss: 6.5479 - val_acc: 0.1667\n",
      "Epoch 3/500\n",
      "210/210 [==============================] - 0s 47us/step - loss: 1.3330e-05 - acc: 1.0000 - val_loss: 6.5666 - val_acc: 0.1667\n",
      "Epoch 4/500\n",
      "210/210 [==============================] - 0s 43us/step - loss: 1.2970e-05 - acc: 1.0000 - val_loss: 6.5853 - val_acc: 0.1667\n",
      "Epoch 5/500\n",
      "210/210 [==============================] - 0s 47us/step - loss: 1.2586e-05 - acc: 1.0000 - val_loss: 6.6031 - val_acc: 0.1667\n",
      "Epoch 6/500\n",
      "210/210 [==============================] - 0s 47us/step - loss: 1.2265e-05 - acc: 1.0000 - val_loss: 6.6215 - val_acc: 0.1667\n",
      "Epoch 7/500\n",
      "210/210 [==============================] - 0s 52us/step - loss: 1.1942e-05 - acc: 1.0000 - val_loss: 6.6403 - val_acc: 0.1667\n",
      "Epoch 8/500\n",
      "210/210 [==============================] - 0s 76us/step - loss: 1.1621e-05 - acc: 1.0000 - val_loss: 6.6585 - val_acc: 0.1667\n",
      "Epoch 9/500\n",
      "210/210 [==============================] - 0s 52us/step - loss: 1.1298e-05 - acc: 1.0000 - val_loss: 6.6761 - val_acc: 0.1667\n",
      "Epoch 10/500\n",
      "210/210 [==============================] - 0s 47us/step - loss: 1.0991e-05 - acc: 1.0000 - val_loss: 6.6934 - val_acc: 0.1667\n",
      "Epoch 11/500\n",
      "210/210 [==============================] - 0s 52us/step - loss: 1.0710e-05 - acc: 1.0000 - val_loss: 6.7109 - val_acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 \n",
    "epochs = 500 \n",
    "\n",
    "num_classes =2\n",
    "\n",
    "node_laters = [100, 100, 100]\n",
    "history_basic = model.fit(r, y_train, \n",
    "                          callbacks=[earlyStopping], \n",
    "                          batch_size=batch_size, \n",
    "                          epochs=epochs, \n",
    "                          validation_split=.3, \n",
    "                          verbose=True)#False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base:\t 0.750\n",
      "log:\t 0.850\n",
      "sgd:\t 0.847\n",
      "svc:\t 0.843\n"
     ]
    }
   ],
   "source": [
    "# baseline accuracy of naive model of predict \"Drake\" on every song. \n",
    "base=d_corp_cnt/len(y_train)\n",
    "print(\"base:\\t {:.3f}\".format(base))\n",
    "for model in score_dict: \n",
    "    print(\"{}:\\t {:.3f}\".format(model, score_dict[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psuedo stacking method of prediction on test set. \n",
    "drake_cols=['sgd_drake', 'log_drake', 'svc_drake', 'rdf_drake', 'ada_drake', 'gdb_drake']\n",
    "quen_cols=['sgd_quen', 'log_quen', 'svc_quen', 'rdf_quen', 'ada_quen', 'gdb_quen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dataframe, and first co\n",
    "# sum the rake probabilities weighted by how much improvement over the the baseline prediction that model offered. \n",
    "# standardize the result so the final output is a probability that will sum to one when added with Quentin probability\n",
    "def stack(df, cols): \n",
    "    result=np.zeros(len(iyrtitl))\n",
    "    for col in cols: \n",
    "#         print(df[col]) #, score_dict[col[:3]])\n",
    "        result += df[col]\n",
    "#     *score_dict[col[:3]]\n",
    "#     score_dict[(col[:3])]*\n",
    "#     accum= 0\n",
    "#     for el in score_dict: \n",
    "#         accum+=score_dict[el]\n",
    "#     print(accum)\n",
    "    return(result/len(cols))\n",
    "#            /accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.81"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum= 0\n",
    "for el in score_dict: \n",
    "    accum+=score_dict[el]\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stake_drake']=stack(df, drake_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stake_quen']=stack(df, quen_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "      <th>log_drake</th>\n",
       "      <th>log_quen</th>\n",
       "      <th>sgd_drake</th>\n",
       "      <th>sgd_quen</th>\n",
       "      <th>svc_drake</th>\n",
       "      <th>svc_quen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Legend</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.578448</td>\n",
       "      <td>0.421552</td>\n",
       "      <td>0.802223</td>\n",
       "      <td>0.197777</td>\n",
       "      <td>0.952967</td>\n",
       "      <td>0.047033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510573</td>\n",
       "      <td>0.489427</td>\n",
       "      <td>0.632240</td>\n",
       "      <td>0.367760</td>\n",
       "      <td>0.657906</td>\n",
       "      <td>0.342094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 Bands</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507951</td>\n",
       "      <td>0.492049</td>\n",
       "      <td>0.552988</td>\n",
       "      <td>0.447012</td>\n",
       "      <td>0.617244</td>\n",
       "      <td>0.382756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Know Yourself</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.524265</td>\n",
       "      <td>0.475735</td>\n",
       "      <td>0.704607</td>\n",
       "      <td>0.295393</td>\n",
       "      <td>0.774711</td>\n",
       "      <td>0.225289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Tellin’</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510937</td>\n",
       "      <td>0.489063</td>\n",
       "      <td>0.626998</td>\n",
       "      <td>0.373002</td>\n",
       "      <td>0.625399</td>\n",
       "      <td>0.374601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Madonna</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530454</td>\n",
       "      <td>0.469546</td>\n",
       "      <td>0.634039</td>\n",
       "      <td>0.365961</td>\n",
       "      <td>0.775260</td>\n",
       "      <td>0.224740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 God</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544712</td>\n",
       "      <td>0.455288</td>\n",
       "      <td>0.763759</td>\n",
       "      <td>0.236241</td>\n",
       "      <td>0.896706</td>\n",
       "      <td>0.103294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Star67</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536065</td>\n",
       "      <td>0.463935</td>\n",
       "      <td>0.739343</td>\n",
       "      <td>0.260657</td>\n",
       "      <td>0.838215</td>\n",
       "      <td>0.161785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preach</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549942</td>\n",
       "      <td>0.450058</td>\n",
       "      <td>0.748440</td>\n",
       "      <td>0.251560</td>\n",
       "      <td>0.903559</td>\n",
       "      <td>0.096441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Used To</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534641</td>\n",
       "      <td>0.465359</td>\n",
       "      <td>0.765621</td>\n",
       "      <td>0.234379</td>\n",
       "      <td>0.829249</td>\n",
       "      <td>0.170751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Man</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557865</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.865656</td>\n",
       "      <td>0.134344</td>\n",
       "      <td>0.935251</td>\n",
       "      <td>0.064749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Now &amp; Forever</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544262</td>\n",
       "      <td>0.455738</td>\n",
       "      <td>0.645744</td>\n",
       "      <td>0.354256</td>\n",
       "      <td>0.896115</td>\n",
       "      <td>0.103885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580268</td>\n",
       "      <td>0.419732</td>\n",
       "      <td>0.909287</td>\n",
       "      <td>0.090713</td>\n",
       "      <td>0.962940</td>\n",
       "      <td>0.037060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You &amp; The 6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555085</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.842379</td>\n",
       "      <td>0.157621</td>\n",
       "      <td>0.932731</td>\n",
       "      <td>0.067269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602617</td>\n",
       "      <td>0.397383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985682</td>\n",
       "      <td>0.014318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PM in New York</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.556189</td>\n",
       "      <td>0.443811</td>\n",
       "      <td>0.823595</td>\n",
       "      <td>0.176405</td>\n",
       "      <td>0.921589</td>\n",
       "      <td>0.078411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How Bout Now</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552799</td>\n",
       "      <td>0.447201</td>\n",
       "      <td>0.726701</td>\n",
       "      <td>0.273299</td>\n",
       "      <td>0.912571</td>\n",
       "      <td>0.087429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>My Side</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.408799</td>\n",
       "      <td>0.971588</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.984667</td>\n",
       "      <td>0.015333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 credits  log_drake  log_quen  sgd_drake  sgd_quen  svc_drake  \\\n",
       "Legend               1.0   0.578448  0.421552   0.802223  0.197777   0.952967   \n",
       "Energy               0.0   0.510573  0.489427   0.632240  0.367760   0.657906   \n",
       "10 Bands             1.0   0.507951  0.492049   0.552988  0.447012   0.617244   \n",
       "Know Yourself        1.0   0.524265  0.475735   0.704607  0.295393   0.774711   \n",
       "No Tellin’           0.0   0.510937  0.489063   0.626998  0.373002   0.625399   \n",
       "Madonna              0.0   0.530454  0.469546   0.634039  0.365961   0.775260   \n",
       "6 God                0.0   0.544712  0.455288   0.763759  0.236241   0.896706   \n",
       "Star67               0.0   0.536065  0.463935   0.739343  0.260657   0.838215   \n",
       "Preach               0.0   0.549942  0.450058   0.748440  0.251560   0.903559   \n",
       "Used To              1.0   0.534641  0.465359   0.765621  0.234379   0.829249   \n",
       "6 Man                0.0   0.557865  0.442135   0.865656  0.134344   0.935251   \n",
       "Now & Forever        0.0   0.544262  0.455738   0.645744  0.354256   0.896115   \n",
       "Company              0.0   0.580268  0.419732   0.909287  0.090713   0.962940   \n",
       "You & The 6          0.0   0.555085  0.444915   0.842379  0.157621   0.932731   \n",
       "Jungle               0.0   0.602617  0.397383   1.000000  0.000000   0.985682   \n",
       "6PM in New York      0.0   0.556189  0.443811   0.823595  0.176405   0.921589   \n",
       "How Bout Now         0.0   0.552799  0.447201   0.726701  0.273299   0.912571   \n",
       "My Side              0.0   0.591201  0.408799   0.971588  0.028412   0.984667   \n",
       "\n",
       "                 svc_quen  \n",
       "Legend           0.047033  \n",
       "Energy           0.342094  \n",
       "10 Bands         0.382756  \n",
       "Know Yourself    0.225289  \n",
       "No Tellin’       0.374601  \n",
       "Madonna          0.224740  \n",
       "6 God            0.103294  \n",
       "Star67           0.161785  \n",
       "Preach           0.096441  \n",
       "Used To          0.170751  \n",
       "6 Man            0.064749  \n",
       "Now & Forever    0.103885  \n",
       "Company          0.037060  \n",
       "You & The 6      0.067269  \n",
       "Jungle           0.014318  \n",
       "6PM in New York  0.078411  \n",
       "How Bout Now     0.087429  \n",
       "My Side          0.015333  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-372-d7794336e74b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "total_tokens[-1].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90,) (30,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-366-8a6b319cd02d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml_drake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrake_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ml_quen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquentin_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ml_tot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-366-8a6b319cd02d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml_drake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrake_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ml_quen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquentin_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ml_tot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__radd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0muse_numexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_numexpr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_bool_arith_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\research\\lib\\site-packages\\pandas\\core\\ops\\roperator.py\u001b[0m in \u001b[0;36mradd\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mradd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (90,) (30,) "
     ]
    }
   ],
   "source": [
    "import functools\n",
    "# def app(ls): \n",
    "l_drake=functools.reduce(lambda x, y : x+y, drake_tokens, [])\n",
    "l_quen=functools.reduce(lambda x, y : x+y, quentin_tokens, [])\n",
    "l_tot=functools.reduce(lambda x, y : x+y, total_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'oh': 3,\n",
       "         're': 3,\n",
       "         'are': 3,\n",
       "         'night': 3,\n",
       "         'miss': 2,\n",
       "         'ever': 3,\n",
       "         'dog': 2,\n",
       "         'wishin': 2,\n",
       "         'whi': 2,\n",
       "         'littl': 1,\n",
       "         'onli': 2,\n",
       "         'firm': 1,\n",
       "         'wanna': 2,\n",
       "         'babi': 3,\n",
       "         'drake': 3,\n",
       "         've': 1,\n",
       "         'other': 2,\n",
       "         'stori': 1,\n",
       "         'bounc': 1,\n",
       "         'where': 1,\n",
       "         'rememb': 1,\n",
       "         'uh': 1,\n",
       "         'who': 1,\n",
       "         'nah': 1})"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(l_drake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'real': 1,\n",
       "         'away': 1,\n",
       "         'forreal': 2,\n",
       "         'live': 3,\n",
       "         'wait': 3,\n",
       "         'bitch': 2,\n",
       "         'control': 2,\n",
       "         'might': 2,\n",
       "         'case': 3,\n",
       "         'came': 2,\n",
       "         'gon': 3,\n",
       "         'woah': 2,\n",
       "         'whoa': 3,\n",
       "         'nike': 3,\n",
       "         'yuh': 3,\n",
       "         'em': 1,\n",
       "         'hit': 1,\n",
       "         'sir': 1,\n",
       "         'lit': 2,\n",
       "         'quentin': 1,\n",
       "         'rap': 1,\n",
       "         'miller': 1,\n",
       "         'beat': 1,\n",
       "         'deep': 1})"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(l_quen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'real': 1,\n",
       "         'away': 1,\n",
       "         'forreal': 2,\n",
       "         'live': 3,\n",
       "         'wait': 3,\n",
       "         'bitch': 2,\n",
       "         'control': 2,\n",
       "         'might': 2,\n",
       "         'case': 3,\n",
       "         'came': 2,\n",
       "         'gon': 3,\n",
       "         'woah': 2,\n",
       "         'whoa': 3,\n",
       "         'nike': 3,\n",
       "         'yuh': 3,\n",
       "         'em': 1,\n",
       "         'hit': 1,\n",
       "         'sir': 1,\n",
       "         'lit': 2,\n",
       "         'quentin': 1,\n",
       "         'rap': 1,\n",
       "         'miller': 1,\n",
       "         'beat': 1,\n",
       "         'deep': 1})"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(l_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['oh',\n",
       "  're',\n",
       "  'are',\n",
       "  'night',\n",
       "  'miss',\n",
       "  'ever',\n",
       "  'dog',\n",
       "  'wishin',\n",
       "  'whi',\n",
       "  'littl',\n",
       "  'onli',\n",
       "  'firm',\n",
       "  'wanna',\n",
       "  'babi',\n",
       "  'drake'],\n",
       " ['real',\n",
       "  'away',\n",
       "  'forreal',\n",
       "  'live',\n",
       "  'wait',\n",
       "  'bitch',\n",
       "  'control',\n",
       "  'might',\n",
       "  'case',\n",
       "  'came',\n",
       "  'gon',\n",
       "  'woah',\n",
       "  'whoa',\n",
       "  'nike',\n",
       "  'yuh'],\n",
       " ['re',\n",
       "  'are',\n",
       "  'oh',\n",
       "  'drake',\n",
       "  've',\n",
       "  'night',\n",
       "  'ever',\n",
       "  'babi',\n",
       "  'other',\n",
       "  'stori',\n",
       "  'dog',\n",
       "  'bounc',\n",
       "  'where',\n",
       "  'rememb',\n",
       "  'uh'],\n",
       " ['em',\n",
       "  'hit',\n",
       "  'wait',\n",
       "  'sir',\n",
       "  'gon',\n",
       "  'whoa',\n",
       "  'case',\n",
       "  'lit',\n",
       "  'quentin',\n",
       "  'rap',\n",
       "  'miller',\n",
       "  'beat',\n",
       "  'nike',\n",
       "  'live',\n",
       "  'yuh'],\n",
       " ['re',\n",
       "  'oh',\n",
       "  'ever',\n",
       "  'wishin',\n",
       "  'are',\n",
       "  'night',\n",
       "  'miss',\n",
       "  'whi',\n",
       "  'babi',\n",
       "  'drake',\n",
       "  'wanna',\n",
       "  'other',\n",
       "  'who',\n",
       "  'onli',\n",
       "  'nah'],\n",
       " ['deep',\n",
       "  'lit',\n",
       "  'forreal',\n",
       "  'bitch',\n",
       "  'live',\n",
       "  'came',\n",
       "  'wait',\n",
       "  'control',\n",
       "  'gon',\n",
       "  'might',\n",
       "  'case',\n",
       "  'woah',\n",
       "  'whoa',\n",
       "  'nike',\n",
       "  'yuh'],\n",
       " Index(['ayi', 'oh', 'where', 'let', 'swear', 'who', 'daughter', 'alway',\n",
       "        'give', 'check', 'rap', 'yuh', 'ever', 'whi', 'would', 've', 'about',\n",
       "        'gon', 'stori', 'wanna', 'onli', 'could', 'drake', 'hit', 'night',\n",
       "        'are', 'live', 'other', 're', 'nike'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
